{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121b21df",
   "metadata": {
    "id": "c10704f8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import scipy.signal\n",
    "import asyncio\n",
    "import numba as nb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from vmdpy import VMD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "os.chdir('../')\n",
    "os.chdir('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9b50b",
   "metadata": {
    "id": "20bd66c7"
   },
   "source": [
    "## Functions needed for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a636ec8",
   "metadata": {
    "id": "4d930db7"
   },
   "source": [
    "format_text() takes the dataframe and the column index which contains the text that needs to be cleaned, in this case the cleaning process was focussed for Tweets which include removal of special charactors, links and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6710309",
   "metadata": {
    "id": "d81bf78d"
   },
   "outputs": [],
   "source": [
    "def format_text(df,col):\n",
    "      #Remove @ tags\n",
    "      comp_df = df.copy()\n",
    "\n",
    "      # remove all the punctuation\n",
    "      comp_df[col] = comp_df[col].str.replace(r'(@\\w*)','')\n",
    "\n",
    "      #Remove URL\n",
    "      comp_df[col] = comp_df[col].str.replace(r\"http\\S+\", \"\")\n",
    "\n",
    "      #Remove # tag \n",
    "      comp_df[col] = comp_df[col].str.replace('#',\"\")\n",
    "\n",
    "      #Remove all non-character\n",
    "      comp_df[col] = comp_df[col].str.replace(r\"[^a-zA-Z ]\",\"\")\n",
    "\n",
    "      # Remove extra space\n",
    "      comp_df[col] = comp_df[col].str.replace(r'( +)',\" \")\n",
    "      comp_df[col] = comp_df[col].str.strip()\n",
    "\n",
    "      # Change to lowercase\n",
    "      comp_df[col] = comp_df[col].str.lower()\n",
    "      comp_df[col] = comp_df[col].str.replace('httpurl', '')\n",
    "      return comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469af4b",
   "metadata": {
    "id": "b1834328"
   },
   "source": [
    "From the given modes, using the SciPy package the mode containing the maximum energy will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97835ad5",
   "metadata": {
    "id": "82f17ab1"
   },
   "outputs": [],
   "source": [
    "def energy(u):\n",
    "# Estimate PSD `S_xx_welch` at discrete frequencies `f_welch`\n",
    "    f_welch, S_xx_welch = scipy.signal.welch(u)\n",
    "    # Integrate PSD over spectral bandwidth\n",
    "    # to obtain signal power `P_welch`\n",
    "    df_welch = f_welch[1] - f_welch[0]\n",
    "    return np.sum(S_xx_welch) * df_welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d0cdcfc",
   "metadata": {
    "id": "b5c05618"
   },
   "outputs": [],
   "source": [
    "def maxvdm(f):\n",
    "    alpha = 3     \n",
    "    tau = 0            \n",
    "    K = 2       \n",
    "    DC = 0             \n",
    "    init = 1           \n",
    "    tol = 1e-8\n",
    "    u, u_hat, omega = VMD(f, alpha, tau, K, DC, init, tol) \n",
    "    energy_array=[]\n",
    "    for i in u:\n",
    "        energy_array.append(energy(i))\n",
    "    ind = np.argmax(energy_array)\n",
    "    return u[ind]\n",
    "\n",
    "\n",
    "def extract(features):\n",
    "  X = []\n",
    "  for i in features:\n",
    "    X.append(maxvdm(i))\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b8f20",
   "metadata": {
    "id": "fdac9e04"
   },
   "source": [
    "## Training Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d7f8d",
   "metadata": {
    "id": "787989d9"
   },
   "source": [
    "Loading and Pre-Processing the Tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb9fcb60",
   "metadata": {
    "id": "48ef089f"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2db9df13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txhVohfnGiNe",
    "outputId": "86cd43f4-fc87-4c3e-8d0e-b6d4464a59d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'test.tsv', 'train.tsv', 'sample_data']"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6272a8bf",
   "metadata": {
    "id": "2217a258"
   },
   "outputs": [],
   "source": [
    "train = format_text(train,'Text')\n",
    "X = train['Text'].tolist()\n",
    "Y_train = train['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b30dc59",
   "metadata": {
    "id": "5273915b"
   },
   "source": [
    "Converting String Labels into Numeric Values with LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adfe66d3",
   "metadata": {
    "id": "db5bb027"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(Y_train)\n",
    "Y_train = le.transform(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63494173",
   "metadata": {
    "id": "7d4d90ae"
   },
   "source": [
    "Computing the TF-IDF vectors from the given corpus of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc442941",
   "metadata": {
    "id": "b6091245"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edd43f",
   "metadata": {
    "id": "8ca5b2a7"
   },
   "source": [
    "From the functions section, the maxvdm() which extracts K modes and returns the mode with the highest energy is called on every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268af5e4",
   "metadata": {
    "id": "1b55a4ca"
   },
   "outputs": [],
   "source": [
    "X_data = [maxvdm(i) for i in tqdm(features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b6b96f",
   "metadata": {
    "id": "b89ff8fc"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_data)\n",
    "df['l'] = Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38fcec",
   "metadata": {
    "id": "687f4558"
   },
   "source": [
    "## Testing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be445d27",
   "metadata": {
    "id": "ed68cba0"
   },
   "source": [
    "The process of testing is similar to the above-mentioned training method, the same object tfidf and labelencoder must be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2487792a",
   "metadata": {
    "id": "4e85fb56"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/content/test.tsv',sep='\\t',header=None)\n",
    "test = format_text(test,1)\n",
    "X_test = test[1].tolist()\n",
    "Y_test = le.transform(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc53f27",
   "metadata": {
    "id": "60c13f6c"
   },
   "outputs": [],
   "source": [
    "features_test = tfidf.transform(X_test).toarray()\n",
    "X_test = [maxvdm(i) for i in tqdm(features_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045cdbd",
   "metadata": {
    "id": "cf51069f"
   },
   "source": [
    "## Evaluating Model Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fc17ddd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f2e7fe1",
    "outputId": "cb6efd58-fd21-4e9a-aa91-98c1703868cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_data,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9233c25",
   "metadata": {
    "id": "1b07fb0c"
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cd6246f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b9d4da3",
    "outputId": "55aacc37-78fd-4554-d85a-ee681ac0108d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.56      0.64       944\n",
      "           1       0.68      0.84      0.75      1056\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.72      0.70      0.70      2000\n",
      "weighted avg       0.72      0.71      0.70      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90cf5e",
   "metadata": {},
   "source": [
    "## Evaluating Performance on IMdb Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7cd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reviews(df,col):\n",
    "      #Remove @ tags\n",
    "      comp_df = df.copy()\n",
    "\n",
    "      # remove all the punctuation\n",
    "      comp_df[col] = comp_df[col].str.replace(r'(@\\w*)','')\n",
    "\n",
    "      #Remove URL\n",
    "      comp_df[col] = comp_df[col].str.replace(r\"http\\S+\", \"\")\n",
    "\n",
    "      #Remove all non-character\n",
    "      comp_df[col] = comp_df[col].str.replace(r\"[^a-zA-Z ]\",\"\")\n",
    "\n",
    "      # Remove extra space\n",
    "      comp_df[col] = comp_df[col].str.replace(r'( +)',\" \")\n",
    "      comp_df[col] = comp_df[col].str.strip()\n",
    "\n",
    "      # Change to lowercase\n",
    "      comp_df[col] = comp_df[col].str.lower()\n",
    "      comp_df[col] = comp_df[col].str.replace('httpurl', '')\n",
    "      return comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d3c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = pd.read_csv('IMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa10c4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0241b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = format_reviews(imdb_data,'review')\n",
    "X = train['review'].tolist()\n",
    "Y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3c189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(Y)\n",
    "y = le.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a564f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745cd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/50000 [00:06<85:01:14,  6.12s/it]\u001b[A\n",
      "  0%|          | 2/50000 [00:12<84:24:53,  6.08s/it]\u001b[A\n",
      "  0%|          | 3/50000 [00:18<85:52:45,  6.18s/it]\u001b[A\n",
      "  0%|          | 4/50000 [00:29<104:38:46,  7.54s/it]\u001b[A\n",
      "  0%|          | 5/50000 [00:39<114:44:09,  8.26s/it]\u001b[A\n",
      "  0%|          | 6/50000 [00:46<110:38:04,  7.97s/it]\u001b[A\n",
      "  0%|          | 7/50000 [00:54<109:20:57,  7.87s/it]\u001b[A\n",
      "  0%|          | 8/50000 [01:01<106:52:51,  7.70s/it]\u001b[A\n",
      "  0%|          | 9/50000 [01:08<103:40:09,  7.47s/it]\u001b[A\n",
      "  0%|          | 10/50000 [01:14<100:22:39,  7.23s/it]\u001b[A\n",
      "  0%|          | 11/50000 [01:21<99:14:07,  7.15s/it] \u001b[A\n",
      "  0%|          | 12/50000 [01:29<101:06:00,  7.28s/it]\u001b[A\n",
      "  0%|          | 13/50000 [01:37<105:27:59,  7.60s/it]\u001b[A\n",
      "  0%|          | 14/50000 [01:46<108:20:59,  7.80s/it]\u001b[A\n",
      "  0%|          | 15/50000 [01:53<104:35:33,  7.53s/it]\u001b[A\n",
      "  0%|          | 16/50000 [02:00<103:04:52,  7.42s/it]\u001b[A\n",
      "  0%|          | 17/50000 [02:08<107:07:39,  7.72s/it]\u001b[A\n",
      "  0%|          | 18/50000 [02:15<103:40:47,  7.47s/it]\u001b[A\n",
      "  0%|          | 19/50000 [02:22<100:17:09,  7.22s/it]\u001b[A\n",
      "  0%|          | 20/50000 [02:28<97:58:23,  7.06s/it] \u001b[A\n",
      "  0%|          | 21/50000 [02:35<97:54:29,  7.05s/it]\u001b[A\n",
      "  0%|          | 22/50000 [02:44<102:44:12,  7.40s/it]\u001b[A\n",
      "  0%|          | 23/50000 [02:51<100:42:14,  7.25s/it]\u001b[A\n",
      "  0%|          | 24/50000 [02:57<99:08:48,  7.14s/it] \u001b[A\n",
      "  0%|          | 25/50000 [03:05<99:06:57,  7.14s/it]\u001b[A\n",
      "  0%|          | 26/50000 [03:12<100:10:19,  7.22s/it]\u001b[A\n",
      "  0%|          | 27/50000 [03:19<97:52:34,  7.05s/it] \u001b[A\n",
      "  0%|          | 28/50000 [03:25<94:20:06,  6.80s/it]\u001b[A\n",
      "  0%|          | 29/50000 [03:31<93:12:26,  6.71s/it]\u001b[A\n",
      "  0%|          | 30/50000 [03:39<95:57:21,  6.91s/it]\u001b[A\n",
      "  0%|          | 31/50000 [03:46<98:52:33,  7.12s/it]\u001b[A\n",
      "  0%|          | 32/50000 [03:53<97:37:33,  7.03s/it]\u001b[A\n",
      "  0%|          | 33/50000 [03:58<89:35:17,  6.45s/it]\u001b[A\n",
      "  0%|          | 34/50000 [04:04<86:55:08,  6.26s/it]\u001b[A\n",
      "  0%|          | 35/50000 [04:10<86:38:13,  6.24s/it]\u001b[A\n",
      "  0%|          | 36/50000 [04:17<88:57:52,  6.41s/it]\u001b[A\n",
      "  0%|          | 37/50000 [04:23<88:51:43,  6.40s/it]\u001b[A\n",
      "  0%|          | 38/50000 [04:30<89:12:40,  6.43s/it]\u001b[A\n",
      "  0%|          | 39/50000 [04:37<90:08:07,  6.49s/it]\u001b[A\n",
      "  0%|          | 40/50000 [04:43<89:49:19,  6.47s/it]\u001b[A\n",
      "  0%|          | 41/50000 [04:49<89:54:34,  6.48s/it]\u001b[A\n",
      "  0%|          | 42/50000 [04:55<85:41:35,  6.18s/it]\u001b[A\n",
      "  0%|          | 43/50000 [05:00<82:46:55,  5.97s/it]\u001b[A\n",
      "  0%|          | 44/50000 [05:06<82:22:35,  5.94s/it]\u001b[A\n",
      "  0%|          | 45/50000 [05:12<82:13:54,  5.93s/it]\u001b[A\n",
      "  0%|          | 46/50000 [05:20<89:49:57,  6.47s/it]\u001b[A\n",
      "  0%|          | 47/50000 [05:25<84:42:35,  6.10s/it]\u001b[A\n",
      "  0%|          | 48/50000 [05:31<83:57:17,  6.05s/it]\u001b[A\n",
      "  0%|          | 49/50000 [05:38<85:44:07,  6.18s/it]\u001b[A\n",
      "  0%|          | 50/50000 [05:44<87:18:25,  6.29s/it]\u001b[A\n",
      "  0%|          | 51/50000 [05:51<88:29:53,  6.38s/it]\u001b[A\n",
      "  0%|          | 52/50000 [05:57<88:13:01,  6.36s/it]\u001b[A\n",
      "  0%|          | 53/50000 [06:03<85:00:57,  6.13s/it]\u001b[A\n",
      "  0%|          | 54/50000 [06:09<84:10:25,  6.07s/it]\u001b[A\n",
      "  0%|          | 55/50000 [06:15<86:53:06,  6.26s/it]\u001b[A\n",
      "  0%|          | 56/50000 [06:22<90:38:13,  6.53s/it]\u001b[A\n",
      "  0%|          | 57/50000 [06:29<91:59:46,  6.63s/it]\u001b[A\n",
      "  0%|          | 58/50000 [06:36<91:45:32,  6.61s/it]\u001b[A\n",
      "  0%|          | 59/50000 [06:43<92:25:29,  6.66s/it]\u001b[A\n",
      "  0%|          | 60/50000 [06:50<93:41:08,  6.75s/it]\u001b[A\n",
      "  0%|          | 61/50000 [06:57<94:52:23,  6.84s/it]\u001b[A\n",
      "  0%|          | 62/50000 [07:03<91:45:18,  6.61s/it]\u001b[A\n",
      "  0%|          | 63/50000 [07:09<89:05:56,  6.42s/it]\u001b[A\n",
      "  0%|          | 64/50000 [07:15<87:47:08,  6.33s/it]\u001b[A\n",
      "  0%|          | 65/50000 [07:21<87:22:09,  6.30s/it]\u001b[A\n",
      "  0%|          | 66/50000 [07:27<86:27:23,  6.23s/it]\u001b[A\n",
      "  0%|          | 67/50000 [07:33<85:08:04,  6.14s/it]\u001b[A\n",
      "  0%|          | 68/50000 [07:39<86:06:27,  6.21s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "X_data = [maxvdm(i) for i in tqdm(features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a67f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X_data, y, test_size=0.33, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40494177",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VMD_Embed.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit1060d4750c904259afeb7847dfa8ded2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
